Formal Definition: Bi-Semantic Entropy (BSE)

Let Q = {q₁, q₂, …, q_K} be K distinct but semantically equivalent formulations of a question. For each formulation qₖ (1 ≤ k ≤ K):

Generate a large set of N distinct answers Aₖ = {Aₖ₁, Aₖ₂, …, Aₖₙ}.
Using a semantic classifier, assign each answer Aₖᵢ into one of M emergent semantic classes C = {C₁, C₂, …, C_M}. (Note that the same set of semantic classes is used for all question formulations.)
Let nⱼₖ be the number of answers from formulation qₖ that fall into semantic class Cⱼ so that for each k: ∑ⱼ₌₁ᴹ nⱼₖ = N.
Over all K formulations, the global count for class Cⱼ is:
    nⱼ = ∑ₖ₌₁ᴷ nⱼₖ.
The total number of answers across all K formulations is K·N, so define the global probability that a randomly selected answer (from all answers generated) falls into class Cⱼ as: pⱼ = nⱼ/(K·N).
Then the Bi-Semantic Entropy for the set Q is given by the Shannon entropy over these aggregated probabilities:
    BSE(Q) = - ∑ⱼ₌₁ᴹ pⱼ log₂ pⱼ.